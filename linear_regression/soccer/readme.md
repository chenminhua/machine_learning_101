coef 表示的是参数的估计值，也就是通过最小二乘计算出的权重系数。拟合结果中可以看出,守门员的评分与球队成绩居然产生了负相关，后卫的评分则与球队成绩有明显的正相关。

std err 表示的是参数估计的标准误差，标准误差度量的就是估计值偏离真实值的平均程度。

最后两列给出了 95%置信区间。

中间两列中的 t 和 P>|t|都是统计学中的关键指标，它们评估的是拟合结果的统计学意义。t 代表 t 统计量，表示了**参数的估计值和原始假设值之间的偏离程度**。在线性回归中通常会假设待拟合的参数值为 0，此时的 t 统计量就等于估计值除以标准误。当数据中的噪声满足正态分布时，t 统计量就满足 t 分布，**其绝对值越大意味着参数等于 0 的可能性越小，拟合的结果也就越可信。**我们可以看到后卫和前锋对比赛的影响更可信。

P>|t|表示的则是统计学中争议最大的指标——**p 值**。p 值是在当原假设为真时，数据等于观测值或比观测值更为极端的概率。简单地说，p 值表示的是数据与一个给定模型不匹配的程度，**p 值越小，说明数据和原假设的模型越不匹配，也就和计算出的模型越匹配**。在这个例子里，原假设认为待估计的参数等于 0，而接近于 0 的 p 值就意味着计算出的参数值得信任。我们可以看到后卫和前锋对比赛的影响更可信。

R-squared 表示的是 $R ^ 2$ 统计量，也叫作决定系数（coefficient of determination），这个取值在 [0, 1] 之间的数量表示的是输出的变化中能被输入的变化所解释的部分所占的比例。在这个例子里，$R ^ 2 = 0.905$ 意味着回归模型能够通过 $x$ 的变化解释大约 91% 的 $y$ 的变化，这表明回归模型具有良好的准确性，回归后依然不能解释的 9% 就来源于噪声。

$R ^ 2$ 统计量具有单调递增的特性，即使在模型中再添加一些和输出无关的属性，计算出来的 $R ^ 2$ 也不会下降。Adj. R-squared 就是校正版的 $R ^ 2$ 统计量。当模型中增加的变量没有统计学意义时，多余的不相关属性会使校正决定系数下降。校正决定系数体现出的是正则化的思想，它在数值上小于未校正的 $R ^ 2$ 统计量。

利用 OLS 模型可以得到多元回归的结果，可如果对结果加以分析，就会发现一个有趣的现象：**一方面，多元模型的校正决定系数是 0.876，意味着所有位置评分共同解释了输出结果的大部分变化，这也可以从预测值与真实值的散点图上观察出来；可另一方面，只有后卫评分和前锋评分的 p 值低于 0.05，似乎球队的战绩只取决于这两个位置的表现。**

看起来校正决定系数和 p 值给出了自相矛盾的解释，这时就需要观察另外一个重要的指标： F 统计量。

F 统计量主要应用在多元回归中，它检验的原假设是所有待估计的参数都等于 0，这意味着只要有一个参数不等于 0，原假设就被推翻。 **F 统计量越大意味着原假设成立的概率越低，理想的 F 值应该在百千量级。**可在上面的多元回归中， F 统计量仅为 34.57，这就支持了 p 值的结论：**估计出的参数的统计学意义并不明显。**

英超数据集在统计上的非显著性可能源自过小的样本数导致的过拟合，也可能源自不同属性之间的共线性（collinearity）。可在更广泛的意义上，它揭示的却是多元线性回归无法回避的一个本质问题：模型虽然具有足够的精确性，却缺乏关于精确性的合理解释。

假定数据共有 10 个属性，如果只保留 10 个属性中的 5 个用于拟合的话，肯定会有不止一个 5 元属性组能够得到彼此接近的优良性能，可对不同 5 元组的解读方式却会大相径庭。这种现象，就是统计学家莱奥·布雷曼口中的“罗生门”（Rashomon）。布雷曼用这个词来描述最优模型的多重性，以及由此造成的统计建模的艰难处境：当不同的多元线性模型性能相近，却公说公有理婆说婆有理时，到底应该如何选择？

将“罗生门”深挖一步，就是机器学习和统计学在认识论上的差异：统计学讲究的是“知其然，知其所以然”，它不仅要找出数据之间的关联性，还要挖出背后的因果性，给计算出的结果赋予令人信服的解释才是统计的核心。相比之下，机器学习只看重结果，只要模型能够对未知数据做出精确的预测，那这个模型能不能讲得清楚根本不是事儿。

# 正则化

LASSO 将 4 个特征中 2 个的系数缩减为 0，这意味着一半的特征被淘汰掉了，其中就包括倒霉的守门员。在 LASSO 看来，对比赛做出贡献的只有中场和前锋球员，而中场的作用又远远不及前锋——这样的结果是否是对英超注重进攻的直观印象的佐证呢？

和 LASSO 相比，岭回归保留了所有的特征，并给门将的表现赋予了接近于 0 的权重系数，以削弱它对结果的影响，其它的权重系数也和原始多元回归的结果更加接近。

但 LASSO 和岭回归的均方误差都高于普通线性回归的均方误差，LASSO 的性能还要劣于岭回归的性能，这是抑制过拟合和降低误差必然的结果。

# 主成分分析

从结果中可以看出，方差最大的主成分占据了近 4/5 的总方差，前两个主成分的方差之和的比例则超过了 90%。在对数据进行降维时，如果将方差的比例阈值设定为 90%，保留的主成分数目就是 2 个，这说明 2 个主成分已经足以解释输出结果中 90% 的变化。
